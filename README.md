# Topsis-Pretrained-model-text-conversational

##Overview
In the era of deep learning, pretrained models have become indispensable tools across various domains, from natural language processing to computer vision. However, selecting the most suitable pretrained model for a specific task remains a challenging endeavor due to the plethora of available options and the lack of standardized evaluation methodologies.

##Key Features

#Metrics considered:

1. **Accuracy:** Accuracy measures the proportion of correct predictions made by a model over the total number of predictions. It is a fundamental metric for evaluating classification and regression models.

2. **Precision:** Precision quantifies the ratio of true positive predictions to the total number of positive predictions made by a model. It reflects the model's ability to avoid false positives.

3. **Recall:** Recall, also known as sensitivity, measures the proportion of true positive predictions made by a model over the total number of actual positive instances in the data. It reflects the model's ability to identify all relevant instances.

4. **F1 Score:** The F1 score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall, making it useful for evaluating models when there is an imbalance between classes.

5. **BLEU (Bilingual Evaluation Understudy):** BLEU is a metric commonly used to evaluate the quality of machine-translated text by comparing it to one or more reference translations. It measures the overlap in n-grams between the candidate and reference translations.

6. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** ROUGE is a set of metrics used for evaluating the quality of summaries produced by automatic summarization systems. It calculates the overlap between system-generated summaries and reference summaries based on various criteria.

7. **Perplexity:** Perplexity is a metric commonly used to evaluate the performance of language models. It measures how well a language model predicts a sample of text. Lower perplexity indicates better performance, as it suggests that the model is more confident in its predictions.

8. **Response Latency:** Response latency measures the time it takes for a system to generate a response or perform a task in response to a given input. It is particularly important for real-time applications, where low latency is critical for user experience.

9. **Mean Opinion Score (MOS):** MOS is a metric used to evaluate the perceived quality of audio, video, or text generated by a system. It is typically obtained through subjective human ratings, where users assign scores based on their perception of quality.

#Input data:

`data.csv`

<img width="756" alt="Screenshot 2024-02-02 at 11 39 54 AM" src="https://github.com/ThatSpaceCowboy/Topsis-Pretrained-model-text-conversational/assets/41112158/58ba9e4d-a2e1-4a28-8d53-4b633b6e1547">

#Output data:

`102117114-result.csv`

<img width="925" alt="Screenshot 2024-02-02 at 11 41 22 AM" src="https://github.com/ThatSpaceCowboy/Topsis-Pretrained-model-text-conversational/assets/41112158/09b265c3-042c-4423-a98a-6943785ec6b8">


